import math
from gameState import GameState
from debugServer import DebugServer
import pathfinding

class RLLearn_SARSA():
    def __init__(self):
        # self.subscriptions = [MsgType.LIGHT_STATE]
        # super().__init__(addr, port, message_buffers, MsgType, FREQUENCY, self.subscriptions)
        # self.state = None
        # self.grid = copy.deepcopy(grid)

        # self.policy = np.zeros((grid.shape, 4))h
        # self.lr = 0.01
        # self.min_lr = 0.001
        # self.lr_decay = 0.99
        # self.gamma = 1
        # self.eps = 0.1
        # self.eps_decay = 0.999
        # self.min_eps = 0.001
        return

    def stepping():
        #Updating the new state, the reward for the step, whether pacman is done or not
        return
    
    def get_action_random():
        return
        
    def get_action_greedy():
        return
        
    def get_action_epsilon():
        return
    
    def calculate_reward(state1, state2, action):
        #takes a board state, the next state and the action and calculates the ending rewards 
        return 0
    
    def action_to_command():
        return
    
    def train():
        
        # initialize Q(s,a)
        # take a random action
        # update Q(s,a)
        # choose the action maximises Q or a random action according to ∆ê-greedy function
        # repeat step 3 and 4 until the game ends
        # update Q(s,a) where s is the last state before the end, a is the last action taken

        return


    def evaluate():
        return

    